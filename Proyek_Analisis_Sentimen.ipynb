{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPkQ6sieSrNz"
      },
      "source": [
        "## Import Library\n",
        "\n",
        "Di bagian ini, saya akan mengimpor berbagai library yang diperlukan untuk scraping, pemrosesan data, dan pelatihan model deep learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ztR9m02aSss8"
      },
      "outputs": [],
      "source": [
        "# Import library yang dibutuhkan\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google_play_scraper import app, reviews\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf5XKT6_Sw0i"
      },
      "source": [
        "## Scraping Data\n",
        "\n",
        "Di bagian ini, saya akan melakukan scraping data ulasan dari aplikasi Duolingo menggunakan google-play-scraper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epdv1Ud5S81I",
        "outputId": "7e6837b2-e40c-4059-90b6-47c1eb35fd2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping selesai! Data tersimpan dalam duolingo_reviews.csv\n"
          ]
        }
      ],
      "source": [
        "# Scraping 4000 review dari aplikasi Duolingo\n",
        "result, _ = reviews(\n",
        "    'com.duolingo',  # ID aplikasi Duolingo di Play Store\n",
        "    lang='en',        # Bahasa Inggris\n",
        "    country='us',     # Negara US\n",
        "    sort=Sort.NEWEST, # Urutkan berdasarkan yang terbaru\n",
        "    count=4000        # Ambil 4000 ulasan\n",
        ")\n",
        "\n",
        "# Konversi ke DataFrame\n",
        "df = pd.DataFrame(result)\n",
        "\n",
        "# Pilih kolom yang relevan\n",
        "df = df[['userName', 'score', 'content']]\n",
        "\n",
        "# Simpan sebagai CSV\n",
        "df.to_csv('duolingo_reviews.csv', index=False)\n",
        "\n",
        "print(\"Scraping selesai! Data tersimpan dalam duolingo_reviews.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWBAN1QoT1XR"
      },
      "source": [
        "## Preprocessing Data\n",
        "\n",
        "Di sini, saya akan membersihkan teks (menghapus tanda baca, angka, dan stopwords), melakukan tokenisasi, dan melabeli sentimen berdasarkan rating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dphw1JsUVBF",
        "outputId": "290210ef-0753-4bd4-95f5-801d51ca3ca9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing selesai! Data tersimpan dalam duolingo_reviews_cleaned.csv\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv('duolingo_reviews.csv')\n",
        "\n",
        "# Buang data kosong\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Label sentimen berdasarkan rating\n",
        "def label_sentiment(score):\n",
        "    if score >= 4:\n",
        "        return \"Positive\"\n",
        "    elif score == 3:\n",
        "        return \"Neutral\"\n",
        "    else:\n",
        "        return \"Negative\"\n",
        "\n",
        "df['sentiment'] = df['score'].apply(label_sentiment)\n",
        "\n",
        "# Membersihkan teks\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Ubah ke huruf kecil\n",
        "    text = re.sub(r'\\W', ' ', text)  # Hapus karakter spesial\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Hapus stopwords\n",
        "    return text\n",
        "\n",
        "df['cleaned_text'] = df['content'].apply(clean_text)\n",
        "\n",
        "# Simpan hasil preprocessing\n",
        "df.to_csv('duolingo_reviews_cleaned.csv', index=False)\n",
        "\n",
        "print(\"Preprocessing selesai! Data tersimpan dalam duolingo_reviews_cleaned.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "520sMZlPUzZa"
      },
      "source": [
        "## Model Training (LSTM)\n",
        "\n",
        "Pada bagian ini, saya akan menggunakan LSTM untuk pelatihan model analisis sentimen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyeUV2dgU39k",
        "outputId": "2c4a6320-53e6-4f7a-a221-1ba4eba4e02c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 118ms/step - accuracy: 0.8281 - loss: 0.5538 - val_accuracy: 0.8737 - val_loss: 0.4245\n",
            "Epoch 2/5\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 110ms/step - accuracy: 0.8838 - loss: 0.3660 - val_accuracy: 0.8788 - val_loss: 0.3913\n",
            "Epoch 3/5\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 115ms/step - accuracy: 0.9220 - loss: 0.2385 - val_accuracy: 0.8687 - val_loss: 0.4269\n",
            "Epoch 4/5\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 115ms/step - accuracy: 0.9364 - loss: 0.1869 - val_accuracy: 0.8775 - val_loss: 0.4934\n",
            "Epoch 5/5\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 108ms/step - accuracy: 0.9429 - loss: 0.1740 - val_accuracy: 0.8637 - val_loss: 0.5313\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model selesai dilatih dan disimpan sebagai sentiment_model.h5\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "df = pd.read_csv('duolingo_reviews_cleaned.csv')\n",
        "\n",
        "# Pastikan tidak ada NaN atau angka dalam teks\n",
        "df['cleaned_text'] = df['cleaned_text'].astype(str)\n",
        "df = df[df['cleaned_text'].str.strip() != '']\n",
        "\n",
        "# Encode label sentimen ke angka\n",
        "label_mapping = {\"Positive\": 2, \"Neutral\": 1, \"Negative\": 0}\n",
        "df['sentiment_encoded'] = df['sentiment'].map(label_mapping)\n",
        "\n",
        "# Tokenisasi teks\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(df['cleaned_text'])  # ‚úÖ Tidak akan error lagi\n",
        "sequences = tokenizer.texts_to_sequences(df['cleaned_text'])\n",
        "\n",
        "# Padding sequence\n",
        "X = pad_sequences(sequences, maxlen=100)\n",
        "y = df['sentiment_encoded'].values\n",
        "\n",
        "# Split data (80% training, 20% testing)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model LSTM\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=128, input_length=100),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    LSTM(64),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(3, activation='softmax')  # Output 3 kelas (Negatif, Netral, Positif)\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Latih model\n",
        "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Simpan model\n",
        "model.save('sentiment_model.h5')\n",
        "print(\"Model selesai dilatih dan disimpan sebagai sentiment_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fryYVoBYVKKH"
      },
      "source": [
        "## Evaluation & Inference\n",
        "\n",
        "Setelah pelatihan, saya akan melakukan evaluasi model dengan data testing dan memberikan output klasifikasi sentimen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDU7zAOsVNZq",
        "outputId": "8eed9ebd-36de-477c-a038-172c4a3f6e2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step\n",
            "Akurasi Model: 86.38%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.41      0.38      0.39        58\n",
            "     Neutral       0.19      0.09      0.12        44\n",
            "    Positive       0.92      0.95      0.93       698\n",
            "\n",
            "    accuracy                           0.86       800\n",
            "   macro avg       0.51      0.47      0.48       800\n",
            "weighted avg       0.84      0.86      0.85       800\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Prediksi testing set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = y_pred.argmax(axis=1)\n",
        "\n",
        "# Evaluasi akurasi\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(f\"Akurasi Model: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Laporan klasifikasi\n",
        "print(classification_report(y_test, y_pred_classes, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BCim2CLVVYw"
      },
      "source": [
        "## Model Testing\n",
        "\n",
        "Testing model dengan testing set untuk memastikan hasil yang optimal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB2GintQVZOo",
        "outputId": "8790a648-e5ad-4233-e956-0bc79e167a5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
            "Akurasi Model: 0.8638\n",
            "Confusion Matrix:\n",
            "[[ 22   6  30]\n",
            " [ 10   4  30]\n",
            " [ 22  11 665]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.41      0.38      0.39        58\n",
            "     Neutral       0.19      0.09      0.12        44\n",
            "    Positive       0.92      0.95      0.93       698\n",
            "\n",
            "    accuracy                           0.86       800\n",
            "   macro avg       0.51      0.47      0.48       800\n",
            "weighted avg       0.84      0.86      0.85       800\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Prediksi pada testing set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Ambil kelas dengan probabilitas tertinggi\n",
        "\n",
        "# Hitung akurasi\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(f'Akurasi Model: {accuracy:.4f}')\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_classes))\n",
        "\n",
        "# Classification Report (Precision, Recall, F1-score)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_classes, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adbILfAjVd7Q"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Di bagian ini, ringkasan dari hasil pelatihan dan evaluasi model yang telah dilakukan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXbXxdpIVlKU",
        "outputId": "a9bfa20f-5c1f-47c8-9982-2cf9de0f9c34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "üìå Conclusion Proyek Analisis Sentimen üìå\n",
            "==================================================\n",
            "üéØ Akurasi Model pada Testing Set: 0.8638\n",
            "üìä Precision: 0.8403\n",
            "üìä Recall: 0.8638\n",
            "üìä F1-Score: 0.8507\n",
            "\n",
            "‚úÖ Model telah memenuhi standar akurasi minimal 85%. Model ini dapat digunakan untuk analisis sentimen dengan performa yang baik.\n"
          ]
        }
      ],
      "source": [
        "# Menampilkan kesimpulan dari hasil pelatihan dan evaluasi model\n",
        "print(\"=\"*50)\n",
        "print(\"üìå Conclusion Proyek Analisis Sentimen üìå\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Menampilkan akurasi testing set\n",
        "print(f\"üéØ Akurasi Model pada Testing Set: {accuracy:.4f}\")\n",
        "\n",
        "# Menampilkan ringkasan classification report\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
        "recall = recall_score(y_test, y_pred_classes, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
        "\n",
        "print(f\"üìä Precision: {precision:.4f}\")\n",
        "print(f\"üìä Recall: {recall:.4f}\")\n",
        "print(f\"üìä F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Kesimpulan\n",
        "if accuracy >= 0.85:\n",
        "    print(\"\\n‚úÖ Model telah memenuhi standar akurasi minimal 85%. Model ini dapat digunakan untuk analisis sentimen dengan performa yang baik.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Model belum mencapai akurasi minimal 85%. Perlu dilakukan optimasi lebih lanjut, seperti meningkatkan dataset, tuning hyperparameter, atau mencoba algoritma lain.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
